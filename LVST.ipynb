{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "# from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "# from keras.optimizers import RMSprop, Adam, SGD\n",
    "# from keras.preprocessing import image\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "# from keras.utils import np_utils\n",
    "# from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "# from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_dir_path='/media/aaron/新加卷1/datasetLVST/train_full_images_0'\n",
    "file_path='/media/aaron/新加卷1/datasetLVST/train_full_labels.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#注意，json模块需要读取的文件对象，给他路径名会报错。。。\n",
    "with open(file_path) as jsonfile:\n",
    "    data = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'points': [[595, 582], [621, 598], [619, 620], [593, 607]], 'illegibility': True, 'transcription': '###'}\n",
      "{'points': [[232, 223], [321, 200], [326, 563], [229, 571]], 'illegibility': False, 'transcription': '高尔夫'}\n",
      "{'points': [[329, 191], [366, 184], [368, 559], [332, 564]], 'illegibility': False, 'transcription': '聚鑫惠通虚拟现实'}\n",
      "{'points': [[213, 577], [387, 564], [389, 597], [212, 609]], 'illegibility': False, 'transcription': 'USG 教学工作室'}\n",
      "{'points': [[217, 612], [379, 601], [379, 629], [217, 638]], 'illegibility': False, 'transcription': '高碑店东区166号'}\n",
      "{'points': [[554, 521], [597, 551], [594, 608], [550, 585]], 'illegibility': False, 'transcription': '连接'}\n"
     ]
    }
   ],
   "source": [
    "for item in data['gt_0']:\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=50\n",
    "img=cv2.imread(os.path.join(file_dir_path,'gt_{}.jpg'.format(index)))\n",
    "for item in data['gt_{}'.format(index)]:\n",
    "    c = (0,255,0)\n",
    "    cv2.line(img,(item['points'][0][0],item['points'][0][1]),(item['points'][1][0],item['points'][1][1]),c,2)\n",
    "    cv2.line(img,(item['points'][1][0],item['points'][1][1]),(item['points'][2][0],item['points'][2][1]),c,2)\n",
    "    cv2.line(img,(item['points'][2][0],item['points'][2][1]),(item['points'][3][0],item['points'][3][1]),c,2)\n",
    "    cv2.line(img,(item['points'][3][0],item['points'][3][1]),(item['points'][0][0],item['points'][0][1]),c,2)\n",
    "#     cv2.putText(img,item['transcription'],(item['points'][0][0],item['points'][0][1]), font, 2,(255,255,255),3)\n",
    "\n",
    "img_PIL = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "font = ImageFont.truetype('NotoSansCJK-Black.ttc', 40)\n",
    "draw = ImageDraw.Draw(img_PIL)\n",
    "\n",
    "for item in data['gt_{}'.format(index)]:    \n",
    "    draw.text((item['points'][0][0],item['points'][0][1]), item['transcription'], font=font, fill=(255,255,255))\n",
    "    # 使用PIL中的save方法保存图片到本地\n",
    "    # img_PIL.save('02.jpg', 'jpeg')\n",
    " \n",
    "    # 转换回OpenCV格式\n",
    "img = cv2.cvtColor(np.asarray(img_PIL),cv2.COLOR_RGB2BGR)\n",
    "cv2.namedWindow('ts', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('ts',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__可以考虑循环这个标签，然后找到是库里的那些标签，然后把他都裁剪出来，然后做数据增强，然后和我们的数据放到一块去训练。这样我们的模型应该是可以提高很多。\n",
    "同时，关于数据增强的部分，通过放大，然后裁剪也是必要的\n",
    "注意关于颜色的选择很值得思考__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__在这个场景文字识别的部分来看，我还是倾向于有效的切割是更为有效的方法，通过大量端对端的学习并非不可，只是对于千万种组合而言，是在是有点难以控制，特别是对于我们这种小的公司而言，可以在后面搞一个文字联想连接的网络，输入几个字来联想成有效的组合，这种组合应该在实际使用场景中是有限的，就像词库一样，这样，分割过程中的识别问题是有可能得到有效的解决的。另外，感觉这种方法也和人类的认知过程比较接近吧，人类也都是去大概的认识一下，然后凭借经验去猜测是一个什么内容，在不同场景下的猜测自然也不同。用给的环境数据建立联想库，然后分割文本，分别识别，吧有瑕疵的文本识别组合起来，输入得到一个该场景下的有效结果。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import cv2\n",
    "import numpy\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    " \n",
    "if __name__ == '__main__':\n",
    " \n",
    "    img_OpenCV = cv2.imread('01.jpg')\n",
    "    # 图像从OpenCV格式转换成PIL格式\n",
    "    img_PIL = Image.fromarray(cv2.cvtColor(img_OpenCV, cv2.COLOR_BGR2RGB))\n",
    " \n",
    "    # 字体  字体*.ttc的存放路径一般是： /usr/share/fonts/opentype/noto/ 查找指令locate *.ttc\n",
    "    font = ImageFont.truetype('NotoSansCJK-Black.ttc', 40)\n",
    "    # 字体颜色\n",
    "    fillColor = (255,0,0)\n",
    "    # 文字输出位置\n",
    "    position = (100,100)\n",
    "    # 输出内容\n",
    "    str = '在图片上输出中文'\n",
    " \n",
    "    # 需要先把输出的中文字符转换成Unicode编码形式\n",
    "    if not isinstance(str, unicode):\n",
    "        str = str.decode('utf8')\n",
    " \n",
    "    draw = ImageDraw.Draw(img_PIL)\n",
    "    draw.text(position, str, font=font, fill=fillColor)\n",
    "    # 使用PIL中的save方法保存图片到本地\n",
    "    # img_PIL.save('02.jpg', 'jpeg')\n",
    " \n",
    "    # 转换回OpenCV格式\n",
    "    img_OpenCV = cv2.cvtColor(numpy.asarray(img_PIL),cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow(\"print chinese to image\",img_OpenCV)\n",
    "    cv2.waitKey()\n",
    "    cv2.imwrite('03.jpg',img_OpenCV)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "665px",
    "left": "1485px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
